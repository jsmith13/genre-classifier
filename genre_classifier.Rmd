---
title: "Genre Classifier"
author: "Jake Smith"
date: "5/17/2019"
output: pdf_document
latex_engine: xelatex
---

# Setup Chunks

```{r setup, message=FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
setwd("C:/Users/Jake/Desktop/Genre Classifier")
set.seed(913535)

# import required libraries
require(dplyr)
require(tidyr)
require(ggplot2)
require(doParallel)
require(glmnet)
require(kknn)
require(randomForest)
require(keras)
```

```{r function definitions}
# function flat.accuracy
# takes two vectors prediction and reference
# calculates the proportion of results in prediction that are equal to those in reference
flat.accuracy <- function(prediction, reference) {
  sum(prediction == reference) / length(reference)
}

# a simple wrapper for the abind function to allow it to work with foreach
array.bind <- function(...) {abind::abind(..., along = 3)}
```

# Data Preparation

```{r define training and test sets}
# set the seed here as well, in case this chunk is accidentally run
set.seed(913535)

# import the dataset
music <- read.csv("final music descriptors.csv")

# split into train and test sets
train <- sample(1:dim(music)[1], ceiling(dim(music))[1]*.7)
train.set <- music[train, ]
test.set <- music[-train, ]

# write training and test sets to disk
save(train.set, file = "train.set")
save(test.set, file = "test.set")
```

```{r create difference features}
# load the training set
load("train.set")

# remove a few observations for which the descriptor calculations appear to have thrown errors
train.set <- train.set[!(train.set$song %in% c("O - Coldplay", "F.O.D. - Green Day", "Living the Laws - Choking Victim", "M1A1 (Lil' Dub Chefin') - Gorillaz", "Call That Gone? - Paul Westerberg", "VCR Wheels - Tyler_ The Creator")), ]

# generate a list of the parent names for the time-series features
features <- unique(sub(pattern = "\\..*", replacement = "", grep("\\..*", colnames(train.set), value = TRUE)))

# reorder the features such that *.1 through *.10 are in order
train.set <- train.set[, c("genre", "song", "album", "length", outer(1:10, features, function(x, y) {paste(y, x, sep = ".")}))]

# for each pair of time-series features n and n+1, compute |n+1 - n|
for (i in 1:length(features)) {
  for (j in 1:9)
    train.set[, paste(features[i], "dif", j, sep = ".")] <- abs(train.set[, paste(features[i], j+1, sep = ".")] - train.set[, paste(features[i], j, sep = ".")])
}
```

```{r transform features}
# log/root transform features with significant right skew
train.set <- mutate_at(train.set, vars(one_of(c("length", 
         outer(c("kurtosis", "Q25", "roughness", "skewness", "zcr"), 1:10, paste, sep = ".")))), 
         function(x) {log(x + .0001)})
train.set <- mutate_at(train.set, vars(one_of(c("length", 
         outer(c("amplitude.dif", "centroid.dif", "entropy.dif", "kurtosis.dif", "Q25.dif", "Q75.dif", "skewness.dif", "zcr.dif"), 1:9, paste, sep = ".")))), 
         function(x) {(x^(1/4))})
train.set <- mutate_at(train.set, vars(one_of(c("length", 
         outer("flatness.dif", 1:9, paste, sep = ".")))), 
         function(x) {(x^(1/2))})
train.set <- mutate_at(train.set, vars(one_of(c("length", 
         outer("roughness.dif", 1:9, paste, sep = ".")))), 
         function(x) {(x^(1/10))})

# power transform features with significant left skew
train.set <- mutate_at(train.set, vars(one_of(
         outer(c("entropy"), 1:10, paste, sep = "."))), 
         function(x) {x^(20)})
train.set <- mutate_at(train.set, vars(one_of(
         outer(c("flatness"), 1:10, paste, sep = "."))), 
         function(x) {x^(4)})
train.set <- mutate_at(train.set, vars(one_of(
         outer(c("Q75"), 1:10, paste, sep = "."))), 
         function(x) {x^(3)})

# scale all numeric features to N(0, 1) distribution
# rebuild dataframe with scaled columns
train.set <- cbind(select(train.set, song, album, genre), sapply(select(train.set, -song, -album, - genre), scale))
```

```{r data formatting function}
# this function duplicates the formatting and transformations done on the train set above
# takes a dataframe x.df
# returns a dataframe in the necessary form to make predictions
format.as.train <- function (x.df) {
  # generate a list of the parent names for the time-series features
  features <- unique(sub(pattern = "\\..*", replacement = "", grep("\\..*", colnames(x.df), value = TRUE)))

  # reorder the features such that *.1 through *.10 are in order
  x.reordered <- x.df[, c("genre", "song", "album", "length", outer(1:10, features, function(x, y) {paste(y, x, sep = ".")}))]
  
  # for each pair of time-series features n and n+1, compute |n+1 - n|
  for (i in 1:length(features)) {
    for (j in 1:9)
      x.reordered[, paste(features[i], "dif", j, sep = ".")] <- 
        abs(x.reordered[, paste(features[i], j+1, sep = ".")] - x.reordered[, paste(features[i], j, sep = ".")])
  }
  
  # log/root transform features with significant right skew
  x.reordered <- mutate_at(x.reordered, vars(one_of(c("length", 
           outer(c("kurtosis", "Q25", "roughness", "skewness", "zcr"), 1:10, paste, sep = ".")))), 
           function(x) {log(x + .0001)})
  x.reordered <- mutate_at(x.reordered, vars(one_of(c("length", 
           outer(c("amplitude.dif", "centroid.dif", "entropy.dif", "kurtosis.dif", "Q25.dif", "Q75.dif", "skewness.dif", "zcr.dif"), 1:9, paste, sep = ".")))), 
           function(x) {(x^(1/4))})
  x.reordered <- mutate_at(x.reordered, vars(one_of(c("length", 
           outer("flatness.dif", 1:9, paste, sep = ".")))), 
           function(x) {(x^(1/2))})
  x.reordered <- mutate_at(x.reordered, vars(one_of(c("length", 
           outer("roughness.dif", 1:9, paste, sep = ".")))), 
           function(x) {(x^(1/10))})
  
  # power transform features with significant left skew
  x.reordered <- mutate_at(x.reordered, vars(one_of(
           outer(c("entropy"), 1:10, paste, sep = "."))), 
           function(x) {x^(20)})
  x.reordered <- mutate_at(x.reordered, vars(one_of(
           outer(c("flatness"), 1:10, paste, sep = "."))), 
           function(x) {x^(4)})
  x.reordered <- mutate_at(x.reordered, vars(one_of(
           outer(c("Q75"), 1:10, paste, sep = "."))), 
           function(x) {x^(3)})
  
  # scale all numeric features to N(0, 1) distribution
  # rebuild dataframe with scaled columns
  x.reordered <- cbind(select(x.reordered, song, album, genre), sapply(select(x.reordered, -song, -album, - genre), scale))
  
  # return the transformed/formatted dataframe
  return(x.reordered)
}
```


```{r feature histograms}
# plot histograms of the individual features
feature.histograms <- ggplot(data = gather(train.set[c("length", grep("\\.2", colnames(train.set), value = TRUE))], key, value)) +
  facet_wrap(~key, scale = "free", ncol = 6) + geom_histogram(aes(x = value), bins = 30) + 
  ggtitle("Distributions of Transformed Features", "Training Set")
```

#Multinomial Regression

```{r tune lasso multinomial regression}
# set seed for reproducible cv group splitting
set.seed(239913)

# divide training set into ten cross-validation groups
cv.groups <- caret::createFolds(train.set$genre, k = 10)

# select values to try for alpha and lambda parameters
alpha.values <- seq(0, 1, 0.05)
lambda.values <- rev(c(seq(1, 10, 2) %o% exp(-20:5)))

# declare a list to hold the training results
glm.training.results <- list()

# declare a parallel processing cluster
cpu.cluster <- makeCluster(3, type = "SOCK")
registerDoParallel(cpu.cluster)

# loop through cross-validation groups
for (i in 1:length(cv.groups)) {
  # split out the current cv group from the rest of the training set
  train.set.test <- train.set[cv.groups[[i]], ]
  train.set.train <- train.set[-cv.groups[[i]], ]
  
  # loop through alpha values; glmnet function fits all lambda values in a single call
  # foreach loop returns an array of predictions with dimensions 'observation' x 'lambda' x 'alpha'
  glm.training.results[[i]] <- foreach (j = 1:length(alpha.values), .combine = array.bind, .packages = c("glmnet", "dplyr")) %dopar% {
    # fit a multinomial regression with elastic net penalization on the 9/10 set
    intermediate.cv.model <- glmnet(x = as.matrix(select(train.set.train, -genre, -song, -album)), 
                                    y = train.set.train$genre, family = "multinomial", 
                                    alpha = alpha.values[j], lambda = lambda.values, standardize = FALSE)
    
    # predict with the fitted model on the 1/10 set
    predict(intermediate.cv.model, as.matrix(select(train.set.test, -genre, -song, -album)), type = "class")
  }
}

# end parallel processing
stopCluster(cpu.cluster)

# save the tuning results
save(glm.training.results, file = "glm.training.results")
```
```{r multinomial regression results processing}
# format the cross-validation results into a single dataframe
# declare an empty array to hold the compiled predictions
glm.training.results.compiled <- array(dim = c(dim(train.set)[1], length(lambda.values), length(alpha.values)))

# convert the list of predictions seperated by cv group into a single array
for (i in 1:length(glm.training.results)) {
  for (j in 1:length(lambda.values)) {
    for (k in 1:length(alpha.values)) {
      glm.training.results.compiled[cv.groups[[i]], j, k] <- glm.training.results[[i]][, j, k]
    }
  }
}

# compute classification accuracy for each condition
glm.tuning.scores <- apply(glm.training.results.compiled, c(2, 3), FUN = function(x) {flat.accuracy(x, ref = train.set$genre)})

# bind the scores into a dataframe with the lambda and alpha values
glm.tuning.scores <- cbind(expand.grid(lambda.values, alpha.values), c(glm.tuning.scores))
colnames(glm.tuning.scores) <- c("lambda", "alpha", "score")

# plot a heatmap of misclassification score as a function of lambda and alpha
ggplot(glm.tuning.scores, aes(log(lambda), alpha, col = score)) + geom_point(shape = 15, size = 4) +
  scale_y_continuous("alpha") + scale_x_continuous("log(lambda)") + ggtitle("LASSO Tuning Parameters")
```


```{r optimized multinomial regression model}
# find determine the best performing parameter set
best.glm.parameters <- glm.tuning.scores[which(glm.tuning.scores$score == max(glm.tuning.scores$score)), ]

# calculate the standard error amongst all fitted models
se <- sd(glm.tuning.scores$score) / sqrt(length(glm.tuning.scores$score))

# find parameter sets with scores within one se of the best performing
# from these find the parameter sets with the maximum lambda penalty
# finally, from these select a random parameter set from those with the highest classification score
optimal.glm.parameters <- glm.tuning.scores[which(glm.tuning.scores$score > as.numeric(best.glm.parameters[3] - se)), ]
optimal.glm.parameters <- optimal.glm.parameters[which(optimal.glm.parameters$lambda == max(optimal.glm.parameters$lambda)), ]
optimal.glm.parameters <- optimal.glm.parameters[sample(which(optimal.glm.parameters$score == max(optimal.glm.parameters$score)), 1), ]

# train a model using the optimized parameters
optimal.glm <- glmnet(x = as.matrix(select(train.set, -genre, -song, -album)), y = train.set$genre, family = "multinomial", lambda = lambda.values, alpha = optimal.glm.parameters$alpha, standardize = FALSE)
```

```{r multinomial regression test set predictions}
# load the test set
load("test.set")

# format the test set as the train set
test.set <- format.as.train(test.set)

# predict classes for the test set
glm.predictions <- predict(optimal.glm, as.matrix(select(test.set, -genre, -song, -album)), 
                           s = optimal.glm.parameters$lambda, type = "class")

# calculate the flat accuracy
flat.accuracy(glm.predictions, test.set$genre)

# tabulate a confusion matrix (predicted versus actual)
table(glm.predictions, test.set$genre)

# clear the workspace, keeping the training set and functions
rm(list = setdiff(ls(), c("train.set", "array.bind", "flat.accuracy", "format.as.train")))
```

# KNN Classifier

```{r tune knn classifier}
# select a few kernals, k-values, and distance parameters to try
kernels <- c("rectangular", "triangular", "epanechnikov", "biweight", "triweight", "cos", "inv", "gaussian", "rank", "optimal")
k.values <- seq(1,21,2)
distances <- c(seq(0.25, 1, 0.25), seq(2, 10, 1))

# declare a parallel processing cluster
cpu.cluster <- makeCluster(3, type = "SOCK")
registerDoParallel(cpu.cluster)

# train a knn model w/ leave-one-out cross-validation for each of the kernal/k/distance combinations
knn.training.results <- foreach(i = 1:length(distances), .packages = c("kknn", "dplyr")) %dopar% {
  set.seed(23945)
  train.kknn(genre ~ ., data = select(train.set, -song, -album), distance = distances[i], ks = k.values, kernel = kernels)
}

# stop parallel sequence
stopCluster(cpu.cluster)

# save the tuning results
save(knn.training.results, file = "knn.training.results")
```
```{r knn tune results}
# loop through the training results, calculating the classification score for each condition
# "flatten" the resulting matrix into a vector
knn.tuning.scores <- as.vector(sapply(knn.training.results, function(x) {sapply(x$fitted.values, function(y) {flat.accuracy(y, ref = train.set$genre)})}))

# bind the scores into a dataframe with the corresponding parameter values
knn.tuning.scores <- cbind(expand.grid(distances, k.values, kernels), knn.tuning.scores)
colnames(knn.tuning.scores) <- c("distance", "k", "kernel", "score")

# plot a heatmap of classification score as a function of kernel and k, facet by Minkowski parameter
ggplot(knn.tuning.scores, aes(k, kernel, fill = score)) + geom_raster() + scale_y_discrete("kernel") +
  scale_x_continuous("k", breaks = seq(1, 101, 10)) + facet_wrap(~distance) + ggtitle("KNN Tuning Parameters")
```
```{r optimized knn model}
# find determine the best performing parameter set
best.knn.parameters <- knn.tuning.scores[which(knn.tuning.scores$score == max(knn.tuning.scores$score)), ]

# calculate the standard error amongst all fitted models
se <- sd(knn.tuning.scores$score) / sqrt(length(knn.tuning.scores$score))

# find parameter sets with scores within one se of the best performing
# from these find the parameter sets with the maximum k-value
# finally, from these select a random parameter set from those with the highest classification score
optimal.knn.parameters <- knn.tuning.scores[which(knn.tuning.scores$score > as.numeric(best.knn.parameters[4] - se)), ]
optimal.knn.parameters <- optimal.knn.parameters[which(optimal.knn.parameters$k == max(optimal.knn.parameters$k)), ]
optimal.knn.parameters <- optimal.knn.parameters[sample(which(optimal.knn.parameters$score == max(optimal.knn.parameters$score)), 1), ]

# train a knn model using the optimized parameters
optimal.knn <- train.kknn(genre ~ ., data = select(train.set, -song, -album), distance = optimal.knn.parameters$distance, ks = optimal.knn.parameters$k, kernel = as.character(optimal.knn.parameters$kernel))

# save the optimized model
save(optimal.knn, file = "optimal.knn")
```

```{r knn test set predictions}
# load the test set
load("test.set")

# format the test set as the train set
test.set <- format.as.train(test.set)

# predict classes for the test set
knn.predictions <- predict(optimal.knn, select(test.set, -genre, -song, -album), s = optimal.glm.parameters[[1]])

# calculate the flat accuracy
flat.accuracy(knn.predictions, test.set$genre)

# tabulate a confusion matrix (predicted versus actual)
table(knn.predictions, test.set$genre)

# clear the workspace, keeping the training set and functions
rm(list = setdiff(ls(), c("train.set", "array.bind", "flat.accuracy", "format.as.train")))
```

# Random Forest Classifier

```{r tune random forest - number of trees}
# identify a reasonable number of trees to use while tuning
# select B values to try
b.values <- c(seq(100, 1000, 100), 1500, 2000)

# declare a dataframe to hold predicted values
b.tuning.scores <- cbind(b.values, rep(NA, length(b.values)))
colnames(b.tuning.scores) <- c("B", "score")

# train a random forest on the training set using each value of B
for (i in 1:length(b.values)) {
  # set seed to ensure reprocible variable selection
  set.seed(38245)
  
  # train a random forest
  intermediate.forest <- randomForest(genre ~ ., data = select(train.set, -song, -album), ntree = b.values[i], strata = train.set$genre)
  
  # calculate the classification score for the out-of-bag predictions
  b.tuning.scores[i, 2] <- flat.accuracy(intermediate.forest$predicted, train.set$genre)
}

# save the tuning scores
save(b.tuning.scores, file = "b.tuning.scores")

# plot classification score versus B
ggplot(data = as.data.frame(b.tuning.scores), aes(x = B, y = score)) + geom_point()
```

Will tune remaining parameters with B = 600.

```{r tune random forest}
# tune the number of variables per tree and size of the terminal nodes
# select values to try for m and node size
m.values <- seq(20, 180, 10)
nodes <- seq(1, 100, 10)
rf.tuning.scores <- cbind(expand.grid(m.values, nodes), rep(NA, dim(expand.grid(m.values, nodes))[1]))
colnames(rf.tuning.scores) <- c("m", "nodes", "score")

# declare a parallel processing cluster
cl <- makeCluster(3, type = "SOCK")
registerDoParallel(cl)

# build random forests from the training set using each of the parameter c  ombinations
rf.tuning.scores$score <- foreach (i = 1:dim(rf.tuning.scores)[1], .combine = c, .packages = c("randomForest", "dplyr")) %dopar% {
  # set seed to ensure reproducible variable selection
  set.seed(38245)
  
  # train a random forest
  intermediate.forest <- randomForest(genre ~ ., data = select(train.set, -song, -album), strata = train.set$genre, ntree = 600, mtry = rf.tuning.scores[i, 1], nodesize = rf.tuning.scores[i, 2])
  
  # return the classification score for the out-of-bag predictions
  flat.accuracy(intermediate.forest$predicted, train.set$genre)
}

# end parallel work
stopCluster(cl)

# save the tuning scores
save(rf.tuning.scores, file = "rf.tuning.scores")

# plot a heatmap of classification score as a function of variables per tree and node size
ggplot(rf.tuning.scores, aes(m, nodes, fill = score)) + geom_raster() + scale_y_discrete("nodes") + scale_x_continuous("m") + ggtitle("Random Forest Tuning Parameters")
```

```{r random forest tune results}
# find determine the best performing parameter set
best.rf.parameters <- rf.tuning.scores[which(rf.tuning.scores$score == max(rf.tuning.scores$score)), ]

# calculate the standard error amongst all fitted models
se <- sd(rf.tuning.scores$score) / sqrt(length(rf.tuning.scores$score))

# find parameter sets with scores within one se of the best performing
# from these find the parameter sets with the minimal m parameter
# finally, from these select a random parameter set from those with the highest classification score
optimal.rf.parameters <- rf.tuning.scores[which(rf.tuning.scores$score > as.numeric(best.rf.parameters$score - se)), ]
optimal.rf.parameters <- optimal.rf.parameters[which(optimal.rf.parameters$m == min(optimal.rf.parameters$m)), ]
optimal.rf.parameters <- optimal.rf.parameters[sample(which(optimal.rf.parameters$score == max(optimal.rf.parameters$score)), 1), ]

# train a random forest using the optimized parameters
optimal.rf <- randomForest(genre ~ ., data = select(train.set, -song, -album), strata = train.set$genre, ntree = 2000, mtry = optimal.rf.parameters$m, nodesize = optimal.rf.parameters$nodes)

# save the optimized model
save(optimal.rf, file = "optimal.rf")
```

```{r random forest test set predictions}
# load the test set
load("test.set")

# format the test set as the train set
test.set <- format.as.train(test.set)

# predict classes for the test set
rf.predictions <- predict(optimal.rf, select(test.set, -genre, -song, -album))

# calculate the flat accuracy
flat.accuracy(rf.predictions, test.set$genre)

# tabulate a confusion matrix (predicted versus actual)
table(rf.predictions, test.set$genre)

# clear the workspace, keeping the training set and functions
rm(list = setdiff(ls(), c("train.set", "array.bind", "flat.accuracy", "format.as.train")))
```

# Neural Network

```{r neural network training}
# convert the training set into the matrix format required by keras
train.set.X <- as.matrix(select(train.set, -genre, -song, -album))
train.set.Y <- to_categorical(as.numeric(train.set$genre))

# define the model
model <- keras_model_sequential()
model %>%
  layer_dense(units = 50, activation = "relu", input_shape = 191) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = dim(train.set.Y)[2], activation = "softmax")
  
# compile the model
compile(model, loss = "categorical_crossentropy", optimizer = optimizer_adadelta(), metrics = "accuracy")
  
# fit the model
nnet.training <- fit(model, x = train.set.X, y = train.set.Y, epochs = 80, batch_size = 250, validation_split = 0.2, verbose = 1)
```

```{r neural network test set predictions}
# load the test set
load("test.set")

# format the test set as the train set
test.set <- format.as.train(test.set)

# build a matrix from the test set features
test.set.X <- as.matrix(select(test.set, -genre, -song, -album))

# predict classes for the observations in the test set
nn.predictions <- predict_classes(model, test.set.X)

# calculate the flat accuracy
flat.accuracy(nn.predictions, as.numeric(test.set$genre))

# tabulate a confusion matrix (predicted versus actual)
table(levels(test.set$genre)[nn.predictions], test.set$genre)
```

```{r}
by.album <- data.frame(cbind(test.set$album, test.set$genre, nn.predictions))
colnames(by.album) <- c("album", "genre", "prediction")

by.album <- by.album %>% group_by(album) %>% summarise(album.pred = which.max(tabulate(prediction)), album.genre = which.max(tabulate(genre)))

sum(by.album$album.pred == by.album$album.genre) / dim(by.album)[1]
```

